{
    "metadata": {
        "kernelspec": {
            "name": "python2-spark20", 
            "display_name": "Python 2 with Spark 2.0", 
            "language": "python"
        }, 
        "language_info": {
            "version": "2.7.11", 
            "name": "python", 
            "mimetype": "text/x-python", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "file_extension": ".py", 
            "nbconvert_exporter": "python"
        }
    }, 
    "cells": [
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 1, 
                    "data": {
                        "text/plain": "[Row(_c0=0, name=u'C.Newton', position=u'QB', rushing_yds=43, home=True, opponent=u'NO', stadium_city=u'Charlotte, NC', capacity=u'75,419', surface=u'Voyager Bermuda Grass', roof=u'Open', temp=65, wspd=7),\n Row(_c0=1, name=u'J.Stewart', position=u'RB', rushing_yds=46, home=True, opponent=u'NO', stadium_city=u'Charlotte, NC', capacity=u'75,419', surface=u'Voyager Bermuda Grass', roof=u'Open', temp=65, wspd=7)]"
                    }
                }
            ], 
            "execution_count": 1, 
            "source": "configuration_name = 'os_f02c10e2d3544fe3ae82e368ee5170ef_configs'\nbmos = ibmos2spark.bluemix(sc, credentials, configuration_name)\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true').option('inferschema','true')\\\n  .load(bmos.url('HWNFLDemo', 'fixed_rb_filtered.csv'))\ndf.take(2)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 21, 
                    "data": {
                        "text/plain": "[Row(_c0=0, name=u'C.Newton', teamid=u'CAR', home_team=u'CAR', away_team=u'MIN', rushing_tds=1, receiving_yds=0, position=u'QB', rushing_yds=26, home=True, opponent=u'MIN', stadium_city=u'Charlotte, NC', capacity=u'75,419', surface=u'Voyager Bermuda Grass', roof=u'Open', temp=85, wspd=5, vis=10, pressure=29, dewPt=67.0, precip_total=None, hi=88.0),\n Row(_c0=1, name=u'M.Tolbert', teamid=u'CAR', home_team=u'CAR', away_team=u'MIN', rushing_tds=0, receiving_yds=14, position=u'FB', rushing_yds=5, home=True, opponent=u'MIN', stadium_city=u'Charlotte, NC', capacity=u'75,419', surface=u'Voyager Bermuda Grass', roof=u'Open', temp=85, wspd=5, vis=10, pressure=29, dewPt=67.0, precip_total=None, hi=88.0)]"
                    }
                }
            ], 
            "execution_count": 21, 
            "source": "df = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true').option('inferschema','true')\\\n  .load('fixed_rb_filtered.csv')\ndf.take(2)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 22, 
                    "data": {
                        "text/plain": "4391"
                    }
                }
            ], 
            "execution_count": 22, 
            "source": "df.count()"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 23, 
            "source": "df = df.where(\"not temp is Null\")"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 24, 
            "source": "df = df.withColumn('ishome', df['home'].cast('integer'))"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 25, 
            "source": "df.groupBy(\"position\").count().where('count < 10').createOrReplaceTempView(\"lowPosCount\")"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 26, 
            "source": "df.groupBy(\"name\").count().where('count < 10').createOrReplaceTempView(\"lowNameCount\")"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 27, 
            "source": "df.createOrReplaceTempView(\"allRows\")"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 28, 
            "source": "morePlayers = spark.sql(\"\"\"select * from allRows where \n                                            name not in (select name from lowNameCount) and\n                                            position not in (select position from lowPosCount)\"\"\")"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 29, 
            "source": "#features temp wspd roof surface home stadium_city away_team name position teamid\n#label rushing_yds"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 30, 
            "source": "from pyspark.ml.feature import StringIndexer\n\nnameInd = StringIndexer(inputCol=\"name\", outputCol=\"nameInd\")\nposInd = StringIndexer(inputCol=\"position\", outputCol=\"posInd\")\n#ampmInd = StringIndexer(inputCol=\"ampm\", outputCol=\"ampmInd\")\nroofInd = StringIndexer(inputCol=\"roof\", outputCol=\"roofInd\")\nsurfInd = StringIndexer(inputCol=\"surface\", outputCol=\"surfInd\")\nstadInd = StringIndexer(inputCol=\"stadium_city\", outputCol=\"stadInd\")\nteamInd = StringIndexer(inputCol=\"teamid\", outputCol=\"teamInd\")\noppInd = StringIndexer(inputCol=\"away_team\", outputCol=\"oppInd\")"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 31, 
            "source": "from pyspark.ml.feature import OneHotEncoder\n\nnameEnc = OneHotEncoder(inputCol=\"nameInd\", outputCol=\"nameEnc\")\nposEnc = OneHotEncoder(inputCol=\"posInd\", outputCol=\"posEnc\")\n#ampmEnc = OneHotEncoder(inputCol=\"ampmInd\", outputCol=\"ampmEnc\")\nroofEnc = OneHotEncoder(inputCol=\"roofInd\", outputCol=\"roofEnc\")\nsurfEnc = OneHotEncoder(inputCol=\"surfInd\", outputCol=\"surfEnc\")\nstadEnc = OneHotEncoder(inputCol=\"stadInd\", outputCol=\"stadEnc\")\nteamEnc = OneHotEncoder(inputCol=\"teamInd\", outputCol=\"teamEnc\")\noppEnc = OneHotEncoder(inputCol=\"oppInd\", outputCol=\"oppEnc\")"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 32, 
            "source": "from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAss = VectorAssembler(\n    inputCols=[\"nameEnc\", \"posEnc\", \"roofEnc\", \"surfEnc\", \"stadEnc\", \"temp\", \"wspd\", \"ishome\", \"teamEnc\", \"oppEnc\"],\n    outputCol=\"features\")"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 33, 
            "source": "from pyspark.ml.regression import RandomForestRegressor\n\nrf = RandomForestRegressor(featuresCol=\"features\",labelCol=\"rushing_yds\")"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 34, 
            "source": "from pyspark.ml import Pipeline\n\npipeline = Pipeline(stages=[nameInd,nameEnc,posInd,posEnc,roofInd,roofEnc,surfInd,surfEnc,\n                           stadInd,stadEnc,teamInd,teamEnc,oppInd,oppEnc,vecAss,rf])"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "2017-09-13 16:27:41.642298\n"
                }
            ], 
            "execution_count": null, 
            "source": "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\nimport datetime\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(rf.maxBins, [40,60,80])\\\n    .addGrid(rf.maxDepth, [16,20,25])\\\n    .addGrid(rf.numTrees, [80,100,120])\\\n    .build()\n    \nevaluator = RegressionEvaluator(labelCol=\"rushing_yds\", predictionCol=\"prediction\", metricName=\"rmse\")\n\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=5)\n\nprint datetime.datetime.now()\ncvModel = crossval.fit(morePlayers)\nprint datetime.datetime.now()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Root Mean Squared Error (RMSE) on test data = 22.6746\n"
                }
            ], 
            "execution_count": 38, 
            "source": "rmse = evaluator.evaluate(cvModel.bestModel.transform(morePlayers))\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n#22.6746"
        }, 
        {
            "metadata": {
                "scrolled": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "40\n25\n120\n"
                }
            ], 
            "execution_count": 39, 
            "source": "print cvModel.bestModel.stages[-1]._java_obj.getMaxBins()#<--40\nprint cvModel.bestModel.stages[-1]._java_obj.getMaxDepth()#25-->\nprint cvModel.bestModel.stages[-1]._java_obj.getNumTrees()#120-->"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 40, 
            "source": "#features temp wspd roof surface home stadium_city away_team name position teamid\n#label rushing_yds\ndf = df.select('temp','wspd','roof','surface','ishome','stadium_city','away_team','name','position','teamid','rushing_yds')"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 41, 
            "source": "from pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml import Pipeline\n\nrf = RandomForestRegressor(featuresCol=\"features\",labelCol=\"rushing_yds\",maxBins=60,maxDepth=25,numTrees=80)\npipeline = Pipeline(stages=[nameInd,nameEnc,posInd,posEnc,roofInd,roofEnc,surfInd,surfEnc,\n                           stadInd,stadEnc,teamInd,teamEnc,oppInd,oppEnc,vecAss,rf])\n\nmodel = pipeline.fit(df)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 42, 
                    "data": {
                        "text/plain": "[Row(rushing_yds=26, prediction=30.990729166666664),\n Row(rushing_yds=5, prediction=8.366956569974738),\n Row(rushing_yds=5, prediction=6.2899855277760315),\n Row(rushing_yds=47, prediction=32.01399980970929),\n Row(rushing_yds=22, prediction=29.92765865639912),\n Row(rushing_yds=10, prediction=16.792291666666664),\n Row(rushing_yds=-2, prediction=5.651267773709917),\n Row(rushing_yds=44, prediction=33.0953426656544),\n Row(rushing_yds=59, prediction=35.19739571416388),\n Row(rushing_yds=41, prediction=37.45950587606837)]"
                    }
                }
            ], 
            "execution_count": 42, 
            "source": "predictions = model.transform(df)\npredictions.select(\"rushing_yds\",\"prediction\").take(10)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "['inputDataSchema', 'evaluationMetrics', 'pipelineVersionHref', 'modelVersionHref', 'trainingDataRef', 'pipelineType', 'creationTime', 'lastUpdated', 'label', 'authorEmail', 'trainingDataSchema', 'authorName', 'version', 'modelType', 'runtime', 'evaluationMethod']\n\nmodelType: sparkml-model-2.0\ntrainingDataSchema: {u'fields': [{u'nullable': True, u'type': u'integer', u'name': u'temp', u'metadata': {}}, {u'nullable': True, u'type': u'integer', u'name': u'wspd', u'metadata': {}}, {u'nullable': True, u'type': u'string', u'name': u'roof', u'metadata': {}}, {u'nullable': True, u'type': u'string', u'name': u'surface', u'metadata': {}}, {u'nullable': True, u'type': u'integer', u'name': u'ishome', u'metadata': {}}, {u'nullable': True, u'type': u'string', u'name': u'stadium_city', u'metadata': {}}, {u'nullable': True, u'type': u'string', u'name': u'away_team', u'metadata': {}}, {u'nullable': True, u'type': u'string', u'name': u'name', u'metadata': {}}, {u'nullable': True, u'type': u'string', u'name': u'position', u'metadata': {}}, {u'nullable': True, u'type': u'string', u'name': u'teamid', u'metadata': {}}, {u'nullable': True, u'type': u'integer', u'name': u'rushing_yds', u'metadata': {}}], u'type': u'struct'}\ncreationTime: 2017-09-14 01:00:31.235000+00:00\nmodelVersionHref: https://ibm-watson-ml.mybluemix.net/v2/artifacts/models/7d18e7a5-3d13-4122-bc67-0e6e7404feb0/versions/849b752e-7fee-4dab-8aaf-0d30bc6774c7\nlabel: rushing_yds\n"
                }
            ], 
            "execution_count": 43, 
            "source": "ml_repository_client = MLRepositoryClient(service_path)\nml_repository_client.authorize(username, password)\n\nmodel_artifact = MLRepositoryArtifact(model, training_data=df, name=\"Rush\")\n\nsaved_model = ml_repository_client.models.save(model_artifact)\n\nprint saved_model.meta.available_props()\nprint\nprint \"modelType: \" + saved_model.meta.prop(\"modelType\")\nprint \"trainingDataSchema: \" + str(saved_model.meta.prop(\"trainingDataSchema\"))\nprint \"creationTime: \" + str(saved_model.meta.prop(\"creationTime\"))\nprint \"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\")\nprint \"label: \" + saved_model.meta.prop(\"label\")"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Rush\n"
                }
            ], 
            "execution_count": 44, 
            "source": "loadedModelArtifact = ml_repository_client.models.get(saved_model.uid)\n\nprint str(loadedModelArtifact.name)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": ""
        }
    ], 
    "nbformat_minor": 1, 
    "nbformat": 4
}